"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«
ä¸¦åˆ—å®Ÿè¡Œæ™‚ã®çµ±è¨ˆè¨ˆç®—ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½
"""

import json
import glob
import os
import statistics
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import defaultdict

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import matplotlib.pyplot as plt
    import pandas as pd
    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False


class PerformanceAnalyzer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®çµæœã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, report_directory="."):
        self.report_directory = report_directory
        self.reports = []
        self.load_reports()
    
    def analyze_parallel_execution_logs(self, log_directory: str = "performance_logs") -> Dict[str, Any]:
        """
        ä¸¦åˆ—å®Ÿè¡Œãƒ­ã‚°ã‚’åˆ†æã—ã¦çµ±è¨ˆã‚’è¨ˆç®—
        
        Args:
            log_directory: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåˆ¥çµ±è¨ˆæƒ…å ±
        """
        # JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        log_files = []
        pattern = os.path.join(log_directory, "execution_*.json")
        log_files = glob.glob(pattern)
        
        if not log_files:
            print(f"âš ï¸  {log_directory} ã«execution_*.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        print(f"ğŸ“Š {len(log_files)}å€‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­...")
        
        # å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        all_executions = []
        for log_file in log_files:
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_executions.append(data)
                    print(f"âœ“ èª­ã¿è¾¼ã¿: {os.path.basename(log_file)}")
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ {log_file}: {e}")
        
        if not all_executions:
            return {}
        
        return self._calculate_parallel_statistics(all_executions)
    
    def _calculate_parallel_statistics(self, execution_data: List[Dict]) -> Dict[str, Any]:
        """ä¸¦åˆ—å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆè¨ˆç®—"""
        checkpoint_times = defaultdict(list)
        total_times = []
        execution_info = []
        
        # ãƒ‡ãƒ¼ã‚¿ã®é›†ç´„
        for execution in execution_data:
            total_time = execution.get('total_execution_time', 0)
            total_times.append(total_time)
            
            execution_info.append({
                'test_name': execution.get('test_name', 'unknown'),
                'task_id': execution.get('task_id', 'unknown'),
                'timestamp': execution.get('timestamp', ''),
                'total_time': total_time
            })
            
            checkpoints = execution.get('checkpoints', {})
            for checkpoint_name, checkpoint_data in checkpoints.items():
                if isinstance(checkpoint_data, dict):
                    time_since_last = checkpoint_data.get('time_since_last', 0)
                    total_elapsed = checkpoint_data.get('total_elapsed', 0)
                    
                    checkpoint_times[checkpoint_name].append({
                        'time_since_last': time_since_last,
                        'total_elapsed': total_elapsed,
                        'task_id': execution.get('task_id', 'unknown'),
                        'timestamp': checkpoint_data.get('timestamp', '')
                    })
        
        # çµ±è¨ˆè¨ˆç®—
        result = {
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),
            'total_executions': len(execution_data),
            'execution_details': execution_info,
            'overall_statistics': self._calculate_time_stats(total_times),
            'checkpoint_statistics': {},
            'summary': {}
        }
        
        # å„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®çµ±è¨ˆ
        checkpoint_summary = []
        for checkpoint_name, times in checkpoint_times.items():
            time_since_last_values = [t['time_since_last'] for t in times]
            total_elapsed_values = [t['total_elapsed'] for t in times]
            
            checkpoint_stats = {
                'count': len(times),
                'time_since_last': self._calculate_time_stats(time_since_last_values),
                'total_elapsed': self._calculate_time_stats(total_elapsed_values),
                'details': times
            }
            
            result['checkpoint_statistics'][checkpoint_name] = checkpoint_stats
            
            # ã‚µãƒãƒªãƒ¼ç”¨
            checkpoint_summary.append({
                'name': checkpoint_name,
                'avg_time_since_last': checkpoint_stats['time_since_last'].get('mean', 0),
                'avg_total_elapsed': checkpoint_stats['total_elapsed'].get('mean', 0),
                'count': len(times)
            })
        
        # ã‚µãƒãƒªãƒ¼æƒ…å ±
        result['summary'] = {
            'avg_total_execution_time': result['overall_statistics'].get('mean', 0),
            'total_checkpoints': len(checkpoint_times),
            'checkpoints_summary': sorted(checkpoint_summary, key=lambda x: x['avg_total_elapsed'])
        }
        
        return result
    
    def _calculate_time_stats(self, times: List[float]) -> Dict[str, float]:
        """æ™‚é–“çµ±è¨ˆã®è¨ˆç®—"""
        if not times:
            return {'count': 0}
        
        stats = {
            'count': len(times),
            'min': min(times),
            'max': max(times),
            'sum': sum(times),
            'mean': statistics.mean(times),
            'median': statistics.median(times)
        }
        
        if len(times) > 1:
            stats['std_dev'] = statistics.stdev(times)
        else:
            stats['std_dev'] = 0
            
        return stats
    
    def generate_parallel_analysis_report(self, analysis_result: Dict[str, Any]) -> str:
        """ä¸¦åˆ—å®Ÿè¡Œåˆ†æçµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not analysis_result:
            return "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
        
        lines = []
        lines.append("=" * 80)
        lines.append("ğŸ“Š ä¸¦åˆ—å®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        lines.append("=" * 80)
        
        # åŸºæœ¬æƒ…å ±
        lines.append(f"åˆ†ææ—¥æ™‚: {analysis_result['analysis_timestamp']}")
        lines.append(f"å®Ÿè¡Œå›æ•°: {analysis_result['total_executions']}")
        
        # å…¨ä½“çµ±è¨ˆ
        overall = analysis_result.get('overall_statistics', {})
        if overall.get('count', 0) > 0:
            lines.append("\\nğŸƒ å…¨ä½“å®Ÿè¡Œæ™‚é–“çµ±è¨ˆ:")
            lines.append(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {overall['mean']:.3f}ç§’")
            lines.append(f"  æœ€çŸ­å®Ÿè¡Œæ™‚é–“: {overall['min']:.3f}ç§’")
            lines.append(f"  æœ€é•·å®Ÿè¡Œæ™‚é–“: {overall['max']:.3f}ç§’")
            lines.append(f"  ä¸­å¤®å€¤: {overall['median']:.3f}ç§’")
            lines.append(f"  æ¨™æº–åå·®: {overall['std_dev']:.3f}ç§’")
            lines.append(f"  åˆè¨ˆå®Ÿè¡Œæ™‚é–“: {overall['sum']:.3f}ç§’")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåˆ¥çµ±è¨ˆã‚µãƒãƒªãƒ¼
        summary = analysis_result.get('summary', {})
        checkpoints_summary = summary.get('checkpoints_summary', [])
        if checkpoints_summary:
            lines.append("\\nğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåˆ¥å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
            lines.append("    (ç´¯ç©æ™‚é–“ã®å¹³å‡å€¤é †)")
            for i, cp in enumerate(checkpoints_summary, 1):
                lines.append(f"  {i:2d}. {cp['name']:<30} "
                           f"å¹³å‡: {cp['avg_total_elapsed']:.3f}ç§’ "
                           f"(å‰å›ã‹ã‚‰: {cp['avg_time_since_last']:.3f}ç§’) "
                           f"å®Ÿè¡Œå›æ•°: {cp['count']}")
        
        # è©³ç´°çµ±è¨ˆ
        checkpoints = analysis_result.get('checkpoint_statistics', {})
        if checkpoints:
            lines.append("\\nğŸ“Š ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè©³ç´°çµ±è¨ˆ:")
            
            for checkpoint_name, stats in checkpoints.items():
                lines.append(f"\\n  ğŸ“Œ {checkpoint_name}")
                lines.append(f"     å®Ÿè¡Œå›æ•°: {stats['count']}")
                
                # å‰å›ã‹ã‚‰ã®æ™‚é–“çµ±è¨ˆ
                since_last = stats.get('time_since_last', {})
                if since_last.get('count', 0) > 0:
                    lines.append(f"     å‰å›ã‹ã‚‰ã®æ™‚é–“:")
                    lines.append(f"       å¹³å‡: {since_last['mean']:.3f}ç§’")
                    lines.append(f"       ç¯„å›²: {since_last['min']:.3f}ç§’ ï½ {since_last['max']:.3f}ç§’")
                    lines.append(f"       æ¨™æº–åå·®: {since_last['std_dev']:.3f}ç§’")
                
                # ç´¯ç©æ™‚é–“çµ±è¨ˆ
                total_elapsed = stats.get('total_elapsed', {})
                if total_elapsed.get('count', 0) > 0:
                    lines.append(f"     ç´¯ç©æ™‚é–“:")
                    lines.append(f"       å¹³å‡: {total_elapsed['mean']:.3f}ç§’")
                    lines.append(f"       ç¯„å›²: {total_elapsed['min']:.3f}ç§’ ï½ {total_elapsed['max']:.3f}ç§’")
                    lines.append(f"       æ¨™æº–åå·®: {total_elapsed['std_dev']:.3f}ç§’")
        
        # å®Ÿè¡Œè©³ç´°
        execution_details = analysis_result.get('execution_details', [])
        if execution_details:
            lines.append("\\nğŸ” å®Ÿè¡Œè©³ç´°:")
            for i, detail in enumerate(execution_details, 1):
                lines.append(f"  {i:2d}. ãƒ†ã‚¹ãƒˆ: {detail['test_name']:<20} "
                           f"ã‚¿ã‚¹ã‚¯: {detail['task_id']:<15} "
                           f"å®Ÿè¡Œæ™‚é–“: {detail['total_time']:.3f}ç§’")
        
        lines.append("\\n" + "=" * 80)
        
        return "\\n".join(lines)
    
    def save_parallel_analysis(self, analysis_result: Dict[str, Any], 
                              output_dir: str = "performance_logs") -> tuple:
        """ä¸¦åˆ—åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå½¢å¼ã§ä¿å­˜
        json_filename = f"parallel_analysis_{timestamp}.json"
        json_filepath = os.path.join(output_dir, json_filename)
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False)
        
        # ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã§ä¿å­˜
        report = self.generate_parallel_analysis_report(analysis_result)
        report_filename = f"parallel_report_{timestamp}.txt"
        report_filepath = os.path.join(output_dir, report_filename)
        
        with open(report_filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\\nğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜:")
        print(f"   JSON: {json_filepath}")
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆ: {report_filepath}")
        
        return json_filepath, report_filepath

    def load_reports(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        pattern = os.path.join(self.report_directory, "performance_report_*.json")
        report_files = glob.glob(pattern)
        
        for file_path in report_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                    report['file_path'] = file_path
                    self.reports.append(report)
                    print(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿: {file_path}")
            except Exception as e:
                print(f"ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
    
    def analyze_trends(self):
        """æ™‚ç³»åˆ—ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘ã‚’åˆ†æ"""
        if not HAS_PLOTTING:
            print("matplotlib/pandasãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªåˆ†æã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
            return None, None
            
        if not self.reports:
            print("åˆ†æå¯¾è±¡ã®ãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return None, None
        
        # å…¨ã¦ã®è¨ˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        all_metrics = []
        for report in self.reports:
            for metric in report['metrics']:
                metric['test_session'] = report['test_session']
                all_metrics.append(metric)
        
        if not all_metrics:
            print("åˆ†æå¯¾è±¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")
            return None, None
        
        # DataFrameã«å¤‰æ›
        if HAS_PLOTTING:
            df = pd.DataFrame(all_metrics)
            
            # æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
            df['test_session'] = pd.to_datetime(df['test_session'])
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®å¹³å‡å®Ÿè¡Œæ™‚é–“
            print("\\n=== ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å¹³å‡å®Ÿè¡Œæ™‚é–“ ===")
            action_stats = df.groupby('action')['duration'].agg(['mean', 'std', 'min', 'max', 'count'])
            print(action_stats.round(3))
            
            return df, action_stats
        else:
            # pandasç„¡ã—ã§ã®åŸºæœ¬çµ±è¨ˆ
            action_stats = {}
            actions = set(m['action'] for m in all_metrics)
            
            for action in actions:
                durations = [m['duration'] for m in all_metrics if m['action'] == action]
                action_stats[action] = {
                    'mean': statistics.mean(durations),
                    'std': statistics.stdev(durations) if len(durations) > 1 else 0,
                    'min': min(durations),
                    'max': max(durations),
                    'count': len(durations)
                }
            
            print("\\n=== ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å¹³å‡å®Ÿè¡Œæ™‚é–“ ===")
            for action, stats in action_stats.items():
                print(f"{action}: å¹³å‡={stats['mean']:.3f}s, æ¨™æº–åå·®={stats['std']:.3f}s, "
                      f"æœ€å°={stats['min']:.3f}s, æœ€å¤§={stats['max']:.3f}s, å›æ•°={stats['count']}")
            
            return all_metrics, action_stats
    
    def generate_charts(self, output_dir="performance_charts"):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not HAS_PLOTTING:
            print("matplotlib/pandasãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return
            
        if not self.reports:
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†æ
        df, action_stats = self.analyze_trends()
        
        if df is None or (HAS_PLOTTING and df.empty):
            return
        
        # 1. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å®Ÿè¡Œæ™‚é–“ã®æ£’ã‚°ãƒ©ãƒ•
        plt.figure(figsize=(12, 6))
        action_means = action_stats['mean'].sort_values(ascending=False)
        action_means.plot(kind='bar')
        plt.title('ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å¹³å‡å®Ÿè¡Œæ™‚é–“')
        plt.xlabel('ã‚¢ã‚¯ã‚·ãƒ§ãƒ³')
        plt.ylabel('å®Ÿè¡Œæ™‚é–“ (ç§’)')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'action_duration_bar.png'))
        plt.close()
        
        # 2. æ™‚ç³»åˆ—ã§ã®å®Ÿè¡Œæ™‚é–“æ¨ç§»
        if len(self.reports) > 1:
            plt.figure(figsize=(14, 8))
            
            actions = df['action'].unique()
            colors = plt.cm.Set3(range(len(actions)))
            
            for i, action in enumerate(actions):
                action_data = df[df['action'] == action]
                plt.plot(action_data['test_session'], action_data['duration'], 
                        'o-', label=action, color=colors[i], alpha=0.7)
            
            plt.title('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»ï¼ˆæ™‚ç³»åˆ—ï¼‰')
            plt.xlabel('ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚åˆ»')
            plt.ylabel('å®Ÿè¡Œæ™‚é–“ (ç§’)')
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'performance_trends.png'))
            plt.close()
        
        # 3. å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        plt.figure(figsize=(10, 6))
        for action in df['action'].unique():
            action_data = df[df['action'] == action]['duration']
            plt.hist(action_data, alpha=0.7, label=action, bins=10)
        
        plt.title('å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ')
        plt.xlabel('å®Ÿè¡Œæ™‚é–“ (ç§’)')
        plt.ylabel('é »åº¦')
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'duration_distribution.png'))
        plt.close()
        
        print(f"\\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆ: {output_dir}/")
        print("- action_duration_bar.png: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å¹³å‡å®Ÿè¡Œæ™‚é–“")
        print("- performance_trends.png: æ™‚ç³»åˆ—æ¨ç§»")
        print("- duration_distribution.png: å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ")
    
    def generate_html_report(self, output_file="performance_report.html"):
        """HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.reports:
            return
        
        df, action_stats = self.analyze_trends()
        
        html_content = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .metrics {{ margin: 20px 0; }}
        .metric-item {{ background-color: #f9f9f9; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .chart {{ margin: 20px 0; text-align: center; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>åˆ†æå¯¾è±¡ãƒ¬ãƒãƒ¼ãƒˆæ•°: {len(self.reports)}</p>
    </div>
    
    <div class="metrics">
        <h2>ğŸ“Š ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥çµ±è¨ˆ</h2>
        <table>
            <tr>
                <th>ã‚¢ã‚¯ã‚·ãƒ§ãƒ³</th>
                <th>å¹³å‡æ™‚é–“(ç§’)</th>
                <th>æ¨™æº–åå·®</th>
                <th>æœ€å°æ™‚é–“</th>
                <th>æœ€å¤§æ™‚é–“</th>
                <th>å®Ÿè¡Œå›æ•°</th>
            </tr>
"""
        
        for action, stats in action_stats.iterrows():
            html_content += f"""
            <tr>
                <td>{action}</td>
                <td>{stats['mean']:.3f}</td>
                <td>{stats['std']:.3f}</td>
                <td>{stats['min']:.3f}</td>
                <td>{stats['max']:.3f}</td>
                <td>{stats['count']}</td>
            </tr>
"""
        
        html_content += """
        </table>
    </div>
    
    <div class="chart">
        <h2>ğŸ“ˆ ãƒãƒ£ãƒ¼ãƒˆ</h2>
        <p>ãƒãƒ£ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ performance_charts/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”Ÿæˆã•ã‚Œã¾ã™</p>
    </div>
    
    <div class="metrics">
        <h2>ğŸ” æ¨å¥¨æ”¹å–„ç‚¹</h2>
"""
        
        # æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
        slowest_actions = action_stats.nlargest(3, 'mean')
        for action, stats in slowest_actions.iterrows():
            html_content += f"""
        <div class="metric-item">
            <strong>{action}</strong>: å¹³å‡{stats['mean']:.3f}ç§’
            - {self._get_improvement_suggestion(action, stats['mean'])}
        </div>
"""
        
        html_content += """
    </div>
</body>
</html>
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {output_file}")
    
    def _get_improvement_suggestion(self, action, duration):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¿œã˜ãŸæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = {
            "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹": "ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã®æœ€é©åŒ–ã€CDNåˆ©ç”¨ã‚’æ¤œè¨",
            "ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ç§»å‹•": "ãƒšãƒ¼ã‚¸é–“ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³é€Ÿåº¦æ”¹å–„ã‚’æ¤œè¨",
            "ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†": "èªè¨¼å‡¦ç†ã®æœ€é©åŒ–ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®è¦‹ç›´ã—",
            "å†™çœŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåœ§ç¸®ã€éåŒæœŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè£…",
            "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœç¢ºèª": "UIã®å¿œç­”æ€§æ”¹å–„ã€ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
        }
        
        base_suggestion = suggestions.get(action, "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ¤œè¨")
        
        if duration > 5:
            return f"è¦æ”¹å–„: {base_suggestion}"
        elif duration > 2:
            return f"æ”¹å–„æ¨å¥¨: {base_suggestion}"
        else:
            return "è‰¯å¥½"


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = PerformanceAnalyzer()
    
    # ã¾ãšä¸¦åˆ—å®Ÿè¡Œãƒ­ã‚°ã®åˆ†æã‚’è©¦è¡Œ
    parallel_analysis = analyzer.analyze_parallel_execution_logs("performance_logs")
    
    if parallel_analysis:
        print("ğŸ“Š ä¸¦åˆ—å®Ÿè¡Œãƒ­ã‚°ã‚’åˆ†æã—ã¦ã„ã¾ã™...")
        report = analyzer.generate_parallel_analysis_report(parallel_analysis)
        print(report)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        json_path, report_path = analyzer.save_parallel_analysis(parallel_analysis)
        print(f"\\nâœ… ä¸¦åˆ—å®Ÿè¡Œåˆ†æå®Œäº†!")
        return
    
    # ä¸¦åˆ—å®Ÿè¡Œãƒ­ã‚°ãŒãªã„å ´åˆã¯é€šå¸¸ã®ãƒ¬ãƒãƒ¼ãƒˆåˆ†æ
    if not analyzer.reports:
        print("âš ï¸  åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™:")
        print("  - performance_logs/execution_*.json (ä¸¦åˆ—å®Ÿè¡Œãƒ­ã‚°)")
        print("  - performance_report_*.json (é€šå¸¸ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ)")
        print("\\nãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    print("ğŸ“Š é€šå¸¸ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’åˆ†æã—ã¦ã„ã¾ã™...")
    
    # åˆ†æå®Ÿè¡Œ
    analyzer.analyze_trends()
    
    # ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆï¼ˆmatplotlibãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    try:
        analyzer.generate_charts()
    except Exception as e:
        print(f"ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    
    # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    analyzer.generate_html_report()
    
    print("\\nâœ… åˆ†æå®Œäº†!")


if __name__ == "__main__":
    main()
